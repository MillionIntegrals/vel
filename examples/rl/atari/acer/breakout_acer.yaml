name: 'breakout_acer'

env:
  name: vel.rl.env.classic_atari
  game: 'BreakoutNoFrameskip-v4'


vec_env:
  name: vel.rl.vecenv.subproc
  frame_history: 4  # How many stacked frames go into a single observation


model:
  name: vel.rl.reinforcers.buffered_policy_gradient.q_policy_gradient_model
  backbone:
    name: vel.models.rl.nature_cnn
    input_width: 84
    input_height: 84
    input_channels: 4  # The same as frame_history


reinforcer:
  name: vel.rl.reinforcers.buffered_policy_gradient

  env_roller:
    name: vel.rl.env_roller.replay_q_env_roller
    buffer_capacity: 50_000
    buffer_initial_size: 10_000
    # Because env has a framestack already built-in, save memory by encoding only last frames in the replay buffer
    frame_stack_compensation: 4

  policy_gradient:
    name: vel.rl.reinforcers.buffered_policy_gradient.acer
    entropy_coefficient: 0.01
    q_coefficient: 0.5
    rho_cap: 10.0
    retrace_rho_cap: 1.0

    max_grad_norm: 10.0

    trust_region: false

  number_of_steps: 20 # How many environment steps go into a single batch
  parallel_envs: 16 # How many environments to run in parallel
  discount_factor: 0.99

  experience_replay: 4


optimizer:
  name: vel.optimizers.rmsprop
  lr: 7.0e-4
  alpha: 0.99
#  epsilon: 1.0e-5
  epsilon: 1.0e-3


commands:
  train:
    name: vel.rl.commands.rl_train_command
    total_frames: 1.1e7
    batches_per_epoch: 30
    openai_logging: true

  record:
    name: vel.rl.commands.record_movie_command
    takes: 10
    videoname: 'breakout_vid_{:04}.avi'
    frame_history: 4
    sample_args:
      argmax_sampling: true

  evaluate:
    name: vel.rl.commands.evaluate_env_command
    takes: 100
    frame_history: 4
    sample_args:
      argmax_sampling: true
