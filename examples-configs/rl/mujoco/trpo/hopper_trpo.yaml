name: 'hopper_trpo'

env:
  name: vel.rl.env.mujoco
  game: 'Hopper-v2'
  normalize_returns: true


vec_env:
  name: vel.rl.vecenv.dummy


model:
  name: vel.rl.models.policy_gradient_model_separate

  input_block:
    name: vel.modules.input.normalize_observations
    input_shape: 17

  policy_backbone:
    name: vel.rl.models.backbone.mlp
    input_length: 11
    hidden_layers: [32, 32]
    activation: 'tanh'

  value_backbone:
    name: vel.rl.models.backbone.mlp
    input_length: 11
    hidden_layers: [32, 32]
    activation: 'tanh'


reinforcer:
  name: vel.rl.reinforcers.on_policy_iteration_reinforcer

  algo:
    name: vel.rl.algo.policy_gradient.trpo
    max_kl: 0.01
    cg_iters: 10
    line_search_iters: 10
    improvement_acceptance_ratio: 0.1
    cg_damping: 0.1
    vf_iters: 5
    entropy_coef: 0.0
#    max_grad_norm: 0.5

  env_roller:
    name: vel.rl.env_roller.vec.step_env_roller
    gae_lambda: 0.98 # Generalized Advantage Estimator Lambda parameter


  parallel_envs: 1 # How many environments to run in parallel
  batch_size: 1024 # How many samples can go into the model once
  number_of_steps: 1024 # How many environment steps go into a single batch
#  experience_replay: 10 # How many times to replay the experience

  discount_factor: 0.99 # Discount factor for the rewards


optimizer:
  name: vel.optimizers.adam
  lr: 0.001
  epsilon: 1.0e-8


#scheduler:
#  name: vel.scheduler.linear_batch_scaler


commands:
  train:
    name: vel.rl.commands.rl_train_command
    total_frames: 1.0e6
    batches_per_epoch: 2
    openai_logging: true

  record:
    name: vel.rl.commands.record_movie_command
    takes: 10
    videoname: 'reacher_vid_{:04}.avi'
    sample_args:
      argmax_sampling: true
