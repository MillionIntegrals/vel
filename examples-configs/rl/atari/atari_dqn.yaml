name: 'atari_dqn'


env:
  name: vel.rl.env.classic_atari
  game: !param game = 'BreakoutNoFrameskip-v4'


vec_env:
  name: vel.rl.vecenv.dummy
  frame_history: 4  # How many stacked frames go into a single observation


model:
  name: vel.rl.policy.dqn

  target_update_frequency: 10_000  # After how many batches to update the target network
  discount_factor: 0.99

  epsilon:
    name: vel.function.linear_and_constant
    end_of_interpolation: 0.1
    initial_value: 1.0
    final_value: 0.1

  net:
    name: vel.net.modular
    layers:
      - name: vel.net.layer.input.image_to_tensor
      - name: vel.rl.layer.nature_cnn


reinforcer:
  name: vel.rl.reinforcer.buffered_off_policy_iteration_reinforcer

  env_roller:
    name: vel.rl.env_roller.transition_replay_env_roller

    replay_buffer:
      name: vel.rl.buffer.circular_replay_buffer

      buffer_initial_size: 30_000 # How many samples we need in the buffer before we start using replay buffer
      buffer_capacity: 250_000

      # Because env has a framestack already built-in, save memory by encoding only last frames in the replay buffer
      frame_stack_compensation: true
      frame_history: 4  # How many stacked frames go into a single observation

  rollout_steps: 4 # How many environment steps (per env) to perform per batch of training
  training_steps: 32 # How many environment steps (per env) to perform per training round
  parallel_envs: 1  # Roll out only one env in parallel, just like in DeepMind paper


optimizer:
  name: vel.optimizer.rmsprop
  lr: 2.5e-4
  alpha: 0.95
  momentum: 0.95
  epsilon: 1.0e-1
  max_grad_norm: 0.5


commands:
  train:
    name: vel.rl.command.rl_train_command
    total_frames: 1.1e7  # 11M
    batches_per_epoch: 2500

  record:
    name: vel.rl.command.record_movie_command
    takes: 10
    videoname: 'atari_vid_{:04}.avi'

  evaluate:
    name: vel.rl.command.evaluate_env_command
    takes: 100
    parallel_envs: 1
